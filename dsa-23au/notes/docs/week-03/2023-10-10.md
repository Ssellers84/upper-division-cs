# Week 3
10 October 2023

## Course Notes
Trinity please type here for the first hour:
A computational resources:
1)Memory(Space) 2)Time

Many Algorithms will use different scales depending on what is more efficient.

Question that was asked in Zoom (Jonah Eadie):
Is it also because those statistics will vary across different systems? Where a metric like algorithmic complexity is agnostic to the underlying hardware

Hardware is constantly updating, for example processors, and there may be changes... but the overall learning of algorithms is the same. It is a constant. Or will also imporve at a constant rate.

Orders of Growth (Hiearchy of Functions) that is more efficient than Linear, is called *______* (Too Quiet to Hear this Portion, feel free to fill in the blanks :) )
If one of your algorithms run at an extreme during a function, that usually isn't good. You want steady change, aka aim for the middle, or the constant.
Constant time takes the same amount of time, no matter how many objects. Doesn't scale with the amount of data.

dxxyoungblood (Feel free to correct, couldn't quite hear): Am I thinking about it backwards? Thinking that Exponential functions are more efficient, but Quinn (aqqmin) clarified getting it closer to the constant time is better. In short, lower is better. Constant time is not dependent on the amount of nodes or data.
Simplified explaination of Constant: Having the same running time no matter how big the input grows. 
If resource use, we want constant.
If more data, modest increase, use exponential.

End Of First Hour
----------------------
